# -*- coding: utf-8 -*-
"""ethical_prompt_memory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N8ZnK0DBTdjJ5OsK_5PQQcmz3sRAeJNf
"""

# Load model directly
from transformers import AutoModel
model = AutoModel.from_pretrained("jinaai/jina-embeddings-v3", trust_remote_code=True)

"""## Abstract llm interactions

### Subtask:
Create a wrapper or abstract layer for LLM calls (ethical assessment, intervention generation) to facilitate switching between models like OpenAI and Gemini.

**Reasoning**:
Define the LLMAbstractor class with initialization and methods for ethical assessment and intervention generation, implementing the logic for OpenAI calls as a first step towards abstraction. This addresses instructions 1 through 7 of the subtask.
"""

import json

class LLMAbstractor:
    """
    Abstracts LLM interactions for ethical assessment and intervention generation,
    facilitating switching between different LLM providers.
    """
    def __init__(self, llm_client, model_name="gpt-3.5-turbo"):
        """
        Initializes the LLM abstractor with a specific client and model.

        Args:
            llm_client: An initialized LLM client object (e.g., OpenAI client).
            model_name (str): The name of the LLM model to use.
        """
        self.client = llm_client
        self.model_name = model_name
        print(f"LLMAbstractor initialized with model: {self.model_name}")

    def perform_ethical_assessment(self, prompt):
        """
        Performs ethical assessment of a user prompt using the configured LLM.

        Args:
            prompt (str): The user's prompt to assess.

        Returns:
            tuple: A tuple containing (ethical_score, ethical_reasoning) or (None, error_message).
        """
        print(f"\n--- LLMAbstractor: perform_ethical_assessment ---")
        print(f"Assessing prompt: '{prompt}' using {self.model_name}")

        try:
            messages = [
                {
                    "role": "system",
                    "content": (
                        "You are an academic ethics reviewer evaluating whether a user's prompt is ethically appropriate for use with AI in academic contexts. "
                        "Be critical, impartial, and apply strict standards of academic integrity.\n\n"
                        "Classify the prompt as one of:\n"
                        '- "Ethical": supports responsible inquiry, learning, and dignity\n'
                        '- "Neutral": technically safe, but not ethically meaningful\n'
                        '- "Unethical": promotes deception, manipulation, harm, or dishonesty\n\n"You MUST respond using the following JSON structure:\n"{"intent": "","ethical_classification": "Ethical" | "Neutral" | "Unethical","ethical_score": float,  // 0.0 (very unethical) to 1.0 (fully ethical),"ethical_reasoning": "<1–3 sentence explanation>"}\n\nEXAMPLES:\n\nPrompt: "Can you help me outline my thesis structure?"\n→ intent: " Outline the thesis structure", ethical_classification: "Ethical", ethical_score: 0.95, ethical_reasoning: Supports self-led work and critical thinking.\n\nPrompt: "Write my full literature review with citations."\n→ intent: " Writing reviews", ethical_classification: "Unethical", ethical_score: 0.25, ethical_reasoning: Delegates core authorship and misuses AI.\n\nPrompt: "Summarize this article for me."\n→ intent: " Summarize article", ethical_classification: "Ethical", ethical_score: 0.9, ethical_reasoning: Supports comprehension with responsible guidance.\n\nPrompt: "Generate keywords and an abstract for this title."\n→ intent: " Writing keywords and an abstract", ethical_classification: "Neutral", ethical_score: 0.6, ethical_reasoning: May support writing, but could risk overuse if unfiltered.'
                    ),
                },
                {
                    "role": "user",
                    "content": f"Prompt: {prompt}\n\nEvaluate its ethical implications:",
                },
            ]

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                max_tokens=150,
            )

            ethical_assessment_str = response.choices[0].message.content.strip()
            # print(f"Raw LLM response: {ethical_assessment_str}") # Debug print

            try:
                ethical_assessment = json.loads(ethical_assessment_str)
                ethical_score = ethical_assessment.get("ethical_score", None)
                ethical_reasoning = ethical_assessment.get("ethical_reasoning", "N/A")
                # print(f"Parsed Assessment: Score={ethical_score}, Reason={ethical_reasoning}") # Debug print
                return ethical_score, ethical_reasoning
            except json.JSONDecodeError as e:
                print(f"Error parsing JSON response from LLM: {e}")
                print(f"Raw response: {ethical_assessment_str}")
                return None, f"Error parsing ethical assessment: {ethical_assessment_str}"

        except Exception as e:
            print(f"Error calling LLM API for ethical check: {e}")
            return None, f"Error during ethical check: {e}"

    def generate_intervention_suggestion(self, recent_unethical_prompt=None, trust_level=None, unethical_streak=None, similar_unethical_prompts=None):
        """
        Generates an intervention suggestion using the configured LLM.

        Args:
            recent_unethical_prompt (str, optional): The most recent unethical prompt. Defaults to None.
            trust_level (float, optional): The current trust level. Defaults to None.
            unethical_streak (int, optional): The current unethical streak count. Defaults to None.
            similar_unethical_prompts (list, optional): A list of similar unethical prompts. Defaults to None.

        Returns:
            str: The generated intervention suggestion message or an error message.
        """
        print(f"\n--- LLMAbstractor: generate_intervention_suggestion ---")
        print(f"Generating intervention using {self.model_name}")

        context_info = ""
        if recent_unethical_prompt:
            context_info += f"Your most recent prompt that contributed to this alert was: \"{recent_unethical_prompt}\"\n"
        if trust_level is not None:
             context_info += f"Your current trust level is {trust_level:.2f}.\n"
        if unethical_streak is not None:
             context_info += f"You have an unethical streak of {unethical_streak}.\n"
        if similar_unethical_prompts:
            context_info += "\nHere are some other prompts you've used that are similar in nature:\n- " + "\n- ".join(similar_unethical_prompts)

        intervention_prompt = f"""Based on your recent interactions and ethical usage score, an intervention is suggested.
{context_info}

Please provide a kind, supportive, and encouraging message to the user. Gently remind them about the importance of academic integrity and using AI responsibly as a tool to assist their own learning and work, rather than replacing their effort. Suggest they rephrase their requests to focus on brainstorming, analysis, or understanding concepts. Avoid any judgmental or punitive language. Focus on helping them learn and improve their ethical AI usage. Reference their recent prompts to make the suggestion more relevant.

Your supportive suggestion:"""

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a kind, supportive, and encouraging AI ethics assistant helping users understand responsible AI usage in academic contexts."},
                    {"role": "user", "content": intervention_prompt}
                ],
                max_tokens=300
            )
            suggestion_message = response.choices[0].message.content.strip()
            # print(f"Generated Suggestion: {suggestion_message}") # Debug print
            return suggestion_message

        except Exception as e:
            print(f"Error generating intervention suggestion: {e}")
            return f"An error occurred while generating intervention suggestion: {e}. Please review your recent prompts for ethical concerns and remember the importance of academic integrity."

"""## Simplified usagelogger (multilingual)

### Subtask:
Simplified usagelogger (multilingual)

**Reasoning**:
Define the UsageLogger class with the corrected Jina embedding model identifier and implement the required methods for logging, saving/loading logs and embeddings, generating embeddings, and finding similar usage. This addresses instructions 1 through 9 of the subtask.
"""

import os
import json
import time
from datetime import datetime
import pandas as pd
import numpy as np
import torch
from scipy.spatial import distance
from transformers import AutoModel, AutoTokenizer

# --- Simplified UsageLogger with Hugging Face Embeddings (Multilingual) ---
class UsageLogger:
    """
    Simplified UsageLogger using Hugging Face embeddings for multilingual text.
    Logs usage data and generates/stores vector embeddings using a local model.
    Persists logs and embeddings to JSON files.
    """
    def __init__(self, embeddings_file="usage_embeddings.json", logs_file="usage_logs.json"):
        self.usage_logs = []
        self.usage_embeddings = []
        self.embeddings_file = embeddings_file
        self.logs_file = logs_file

        # Load Hugging Face Embedding Model and Tokenizer
        try:
            # Using the correct model identifier "jinaai/jina-embeddings-v3"
            self.embedding_tokenizer = AutoTokenizer.from_pretrained("jinaai/jina-embeddings-v3")
            # Using the correct model identifier "jinaai/jina-embeddings-v3"
            self.embedding_model = AutoModel.from_pretrained("jinaai/jina-embeddings-v3", trust_remote_code=True)
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.embedding_model.to(self.device)
            print("Jina embedding model loaded successfully.")
        except Exception as e:
            print(f"Error loading Jina embedding model: {e}")
            self.embedding_tokenizer = None
            self.embedding_model = None
            self.device = None

        self._load_logs()
        self._load_embeddings()
        print("Using Simplified UsageLogger with Jina embeddings.")

    # --- Internal Helper Methods for Persistence ---
    def _load_logs(self):
        if os.path.exists(self.logs_file):
            try:
                with open(self.logs_file, 'r') as f:
                    loaded_logs = json.load(f)
                    for log in loaded_logs:
                        if 'timestamp' in log and isinstance(log['timestamp'], str):
                            log['timestamp'] = pd.Timestamp(log['timestamp'])
                    self.usage_logs = loaded_logs
                print(f"Loaded {len(self.usage_logs)} logs from {self.logs_file}")
            except Exception as e:
                print(f"Error loading logs from {self.logs_file}: {e}")
                self.usage_logs = []
        else:
            print(f"Logs file not found: {self.logs_file}. Starting with empty logs.")

    def _save_logs(self):
        try:
            with open(self.logs_file, 'w') as f:
                logs_to_save = []
                for log in self.usage_logs:
                    log_to_save = log.copy()
                    if isinstance(log_to_save.get('timestamp'), pd.Timestamp):
                         log_to_save['timestamp'] = log_to_save['timestamp'].isoformat()
                    logs_to_save.append(log_to_save)
                json.dump(logs_to_save, f, indent=4)
            print(f"Successfully saved {len(self.usage_logs)} logs to {self.logs_file}")
        except Exception as e:
            print(f"Error saving logs to {self.logs_file}: {e}")

    def _load_embeddings(self):
        if os.path.exists(self.embeddings_file):
            try:
                with open(self.embeddings_file, 'r') as f:
                    loaded_embeddings = json.load(f)
                    for item in loaded_embeddings:
                         if 'embedding' in item and isinstance(item['embedding'], list):
                              item['embedding'] = np.array(item['embedding'])
                    self.usage_embeddings = loaded_embeddings
                print(f"Loaded {len(self.usage_embeddings)} embeddings from {self.embeddings_file}")
            except Exception as e:
                print(f"Error loading embeddings from {self.embeddings_file}: {e}")
                self.usage_embeddings = []
        else:
            print(f"Embeddings file not found: {self.embeddings_file}. Starting with empty embeddings.")

    def _save_embeddings(self):
        try:
            embeddings_to_save = []
            for item in self.usage_embeddings:
                 item_to_save = item.copy()
                 if 'embedding' in item_to_save and isinstance(item_to_save['embedding'], np.ndarray):
                      item_to_save['embedding'] = item_to_save['embedding'].tolist()
                 embeddings_to_save.append(item_to_save)

            with open(self.embeddings_file, 'w') as f:
                json.dump(embeddings_to_save, f, indent=4)
            print(f"Saved {len(self.usage_embeddings)} embeddings to {self.embeddings_file}")
        except Exception as e:
            print(f"Error saving embeddings to {self.embeddings_file}: {e}")

    # --- Core Function: log_usage ---
    def log_usage(self, prompt, intent="unknown", thesis_stage="unknown", ethical_score=None, ethical_reason=None):
        """
        Logs a new usage entry, generates its embedding, and saves data.
        """
        print("\n--- Core UsageLogger Function: log_usage ---")
        log_entry = {
            "timestamp": pd.Timestamp.now(),
            "prompt": prompt,
            "thesis_stage": thesis_stage,
            "intent": intent,
            "ethical_score": ethical_score,
            "ethical_reason": ethical_reason,
        }

        self.usage_logs.append(log_entry)
        self._save_logs()

        # Generate and store embedding for the new log entry with 'retrieval.passage' task
        self._generate_embedding_for_log(log_entry, task="retrieval.passage")

    # --- Helper Function: _generate_embedding_for_log ---
    def _generate_embedding_for_log(self, log_entry, task=None):
        """
        Generates an embedding vector for a single log entry's prompt using Jina embeddings.
        """
        if self.embedding_model is None or self.embedding_tokenizer is None or self.device is None:
            print("Jina embedding model not loaded. Cannot generate embedding.")
            return

        try:
            prompt_text = log_entry['prompt']
            # Add the task prefix to the prompt text.
            prompt_text_with_task = f"task:{task} {prompt_text}" if task else prompt_text

            inputs = self.embedding_tokenizer(prompt_text_with_task, return_tensors='pt', padding=True, truncation=True).to(self.device)

            with torch.no_grad():
                outputs = self.embedding_model(**inputs)

            # Get the embeddings (usually the last hidden state, mean pooled)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

            # Store embedding with the index of the original log entry
            self.usage_embeddings.append({'embedding': embedding, 'original_index': len(self.usage_logs) - 1})
            self._save_embeddings()

        except Exception as e:
            print(f"Error generating embedding for log entry: {e}")

    # --- Core Function: find_similar_usage ---
    def find_similar_usage(self, query_prompt, n=3):
        """
        Finds the n most similar usage logs based on prompt embeddings using Jina embeddings.
        """
        print("\n--- Core UsageLogger Function: find_similar_usage ---")
        if not self.usage_embeddings:
            print("No usage embeddings available to query. Please generate embeddings first.")
            return []
        if self.embedding_model is None or self.embedding_tokenizer is None or self.device is None:
             print("Jina embedding model not loaded. Cannot find similar usage.")
             return []

        try:
            # Generate embedding for the query prompt using the Jina model with 'text-matching' task
            # Add the task prefix to the query prompt text.
            query_prompt_with_task = f"task:text-matching {query_prompt}"
            query_inputs = self.embedding_tokenizer(query_prompt_with_task, return_tensors='pt', padding=True, truncation=True).to(self.device)

            with torch.no_grad():
                query_outputs = self.embedding_model(**query_inputs)
            query_embedding = query_outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

            # Calculate cosine distance
            distances = []
            for item in self.usage_embeddings:
                stored_embedding = np.array(item['embedding']) if isinstance(item['embedding'], list) else item['embedding']
                # Handle potential shape mismatch if embeddings are not generated correctly
                if stored_embedding.shape == query_embedding.shape:
                    dist = distance.cosine(query_embedding, stored_embedding)
                    distances.append({
                        "distance": dist,
                        "original_index": item['original_index']
                        })
                else:
                    print(f"Warning: Embedding shape mismatch for log index {item['original_index']}. Skipping.")

            # Sort and get top n
            distances_sorted = sorted(distances, key=lambda x: x['distance'])

            similar_logs = []
            # Retrieve the original log entries
            for item in distances_sorted[0:n]:
                if 0 <= item['original_index'] < len(self.usage_logs):
                     original_log = self.usage_logs[item['original_index']]
                     similar_logs.append({
                         "distance": item['distance'],
                         "original_log": original_log
                     })
                else:
                    print(f"Warning: Original log index {item['original_index']} out of bounds in usage_logs. Skipping this entry.")

            return similar_logs

        except Exception as e:
            print(f"Error during similar usage query: {e}")
            return []

"""## EthicsModule (Multilingual & Abstracted LLM)

### Subtask:
Provide the simplified `EthicsModule`, updating `check_ethical_usage` and `detect_ai` to handle multilingual text and use the abstracted LLM calls.

**Reasoning**:
Define the EthicsModule class with initialization and methods for ethical assessment and AI detection, ensuring they use the LLMAbstractor and can handle multilingual input. This addresses instructions 1 through 6 of the subtask.
"""

import torch
import torch.nn as nn
from transformers import AutoConfig, AutoModel, AutoTokenizer, PreTrainedModel

# Assuming LLMAbstractor, DesklibAIDetectionModel, and predict_single_text are defined in previous cells.

class EthicsModule:
    """
    Handles ethical assessment and AI detection using abstracted LLM calls
    and a local AI detection model. Designed to handle multilingual text.
    """
    def __init__(self, llm_abstractor, ai_detection_model_directory="desklib/ai-text-detector-v1.01"):
        """
        Initializes the EthicsModule with an LLM abstractor and AI detection model.

        Args:
            llm_abstractor (LLMAbstractor): An initialized LLMAbstractor instance.
            ai_detection_model_directory (str): Path or identifier for the AI detection model.
        """
        self.llm_abstractor = llm_abstractor
        print("EthicsModule initialized with LLMAbstractor.")

        # Load Desklib AI Detection Model and Tokenizer
        try:
            self.ai_tokenizer = AutoTokenizer.from_pretrained(ai_detection_model_directory)
            # Using AutoConfig to load config first, then pass to the model class
            config = AutoConfig.from_pretrained(ai_detection_model_directory)
            self.ai_model = DesklibAIDetectionModel.from_pretrained(ai_detection_model_directory, config=config) # Pass config here

            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.ai_model.to(self.device)
            print("Desklib AI detection model loaded successfully.")
        except Exception as e:
            print(f"Error loading Desklib AI detection model: {e}")
            self.ai_tokenizer = None
            self.ai_model = None
            self.device = None

    # --- Core Function: check_ethical_usage ---
    def check_ethical_usage(self, prompt):
        """
        Evaluates the ethical quality of a user prompt using the abstracted LLM.
        Handles multilingual input as the LLMAbstractor is expected to support it.

        Args:
            prompt (str): The user's prompt to assess (can be in any language supported by the LLM).

        Returns:
            tuple: A tuple containing (ethical_score, ethical_reasoning) or (None, error_message).
        """
        print("\n--- Core Function: check_ethical_usage ---")
        # The LLMAbstractor handles the actual LLM call and multilingual aspect
        return self.llm_abstractor.perform_ethical_assessment(prompt)

    # --- Core Function: detect_ai ---
    def detect_ai(self, text):
        """
        Uses the Desklib AI Detection Model to assess if content is AI generated.
        Handles multilingual input as the tokenizer and model are expected to support it.

        Args:
            text (str): The input text to check for AI generation (can be in any language supported by the model).

        Returns:
            tuple: (is_ai_generated, probability, explanation)
        """
        print("\n--- Core Function: detect_ai ---")
        print("Using Desklib AI Detection Model for AI detection...")
        if self.ai_model is None or self.ai_tokenizer is None or self.device is None:
            print("Desklib AI detection model not loaded. Cannot perform AI detection.")
            return False, 0.0, "Error: AI detection model not loaded."

        try:
            # Call predict_single_text and get the result
            prediction_result = predict_single_text(
                text, self.ai_model, self.ai_tokenizer, self.device
            )

            # Check if the result has the expected number of values before unpacking
            if isinstance(prediction_result, tuple) and len(prediction_result) == 3:
                probability, predicted_label, ethics_score_from_ai_detection = prediction_result
                is_ai_generated = predicted_label == 1

                explanation_text = f"AI detection score: {probability:.4f}. Predicted label: {'AI Generated' if is_ai_generated else 'Not AI Generated'}."
                llm_response = explanation_text

                print(f"AI detection score from Desklib model: {probability:.4f}. Is likely AI: {is_ai_generated}")
                if is_ai_generated:
                    print("Ethical Alert (AI Detection): Potential AI-generated content detected. Encourage rephrasing.")
                return is_ai_generated, probability, llm_response
            else:
                # Handle unexpected return format from predict_single_text
                error_message = f"Unexpected return format from predict_single_text: Expected 3 values, got {len(prediction_result) if isinstance(prediction_result, tuple) else type(prediction_result).__name__}"
                print(f"Error during Desklib AI detection: {error_message}")
                return False, 0.0, f"Error: {error_message}"


        except Exception as e:
            print(f"Error during Desklib AI detection: {e}")
            return False, 0.0, f"Error: {e}"

"""## EthicsProfile (Abstracted LLM)

### Subtask:
Provide the simplified `EthicsProfile`, updating `suggest_intervention_standalone` to use the abstracted LLM calls.

**Reasoning**:
Define the EthicsProfile class with initialization and methods for managing the user's ethical state and suggesting interventions, using the LLMAbstractor for intervention generation. This addresses instructions 1 through 6 of the subtask.
"""

# Assuming UsageLogger, LLMAbstractor, and EthicsModule are defined in previous cells.

class EthicsProfile(EthicsModule): # Inherit from EthicsModule to access detect_ai if needed
    """
    Tracks a user's ethical usage history and triggers interventions as needed.
    Uses a UsageLogger to store history and an LLMAbstractor for interventions.
    """
    def __init__(self, logger, llm_abstractor, history_limit=30):
        """
        Initializes the EthicsProfile.

        Args:
            logger (UsageLogger): An initialized UsageLogger instance.
            llm_abstractor (LLMAbstractor): An initialized LLMAbstractor instance.
            history_limit (int): The maximum number of ethical scores to keep in history.
        """
        # Initialize the parent EthicsModule
        # Pass the llm_abstractor to the parent class as it needs it for ethical checks
        super().__init__(llm_abstractor=llm_abstractor)

        self.logger = logger
        self.llm_abstractor = llm_abstractor
        self.history_limit = history_limit

        self.score_history = []
        self.unethical_streak = 0
        self.intervention_triggered = False

        self.trust_level = 0.8
        self.ethical_momentum = 0.0
        print("EthicsProfile initialized.")


    # --- Core Function: update_state ---
    def update_state(self, new_score):
        """
        Update the ethics state based on a new prompt's ethical score.
        Low scores drop trust fast; high scores recover slowly.
        Also triggers reflection mode if threshold crossed.
        """
        print("\n--- Core Function: update_state ---")
        print(f"Updating state with new score: {new_score}")

        if new_score is not None:
            self.score_history.append(new_score)

            if len(self.score_history) > self.history_limit:
                self.score_history.pop(0)

            if len(self.score_history) >= 6:
                recent = self.score_history[-3:]
                past = self.score_history[-6:-3]
                # Ensure past has enough elements to avoid division by zero or index errors
                if len(past) == 3:
                     self.ethical_momentum = round(sum(recent)/3 - sum(past)/3, 2)
                else:
                     self.ethical_momentum = 0.0 # Not enough history for momentum


            if new_score < 0.4:
                penalty = (0.4 - new_score) * 2.5
                self.trust_level -= penalty
                self.unethical_streak += 1
            else:
                gain = (new_score - 0.4) * 0.5
                self.trust_level += gain
                self.unethical_streak = 0

            self.trust_level = max(0.0, min(1.0, round(self.trust_level, 3)))

            self.intervention_triggered = self.unethical_streak >= 3 or self.trust_level < 0.4
        else:
            print("Cannot update state with None score.")


    def show_state(self):
        """Print the current ethical state and any warning flags."""
        print("\n=== ETHICS PROFILE STATE ===")
        print(f"Trust Level: {self.trust_level}")
        print(f"Momentum: {self.ethical_momentum}")
        print(f"Unethical Streak: {self.unethical_streak}")
        print(f"Intervention Needed: {self.intervention_triggered}")
        print("============================")


    # --- Core Function: suggest_intervention_standalone ---
    def suggest_intervention_standalone(self):
        """
        Suggests necessary intervention if intervention_triggered is True.
        Uses the LLMAbstractor to generate a kind and supportive answer.
        Analyzes recent unethical prompts and similar usage for context.
        """
        print("\n--- Core EthicsProfile Function: suggest_intervention_standalone ---")
        if not self.intervention_triggered:
            print("No intervention needed at this time.")
            return {"intervention_suggested": False, "message": "No intervention needed."}

        print("Intervention triggered. Generating suggestion...")

        recent_unethical_prompt = None
        # Find the most recent prompt with a low ethical score from the logger's logs
        # Ensure logger.usage_logs is accessed correctly
        if hasattr(self.logger, 'usage_logs'):
            for entry in reversed(self.logger.usage_logs):
                 # Check if 'ethical_score' exists and is not None before comparison
                if entry.get("ethical_score") is not None and entry["ethical_score"] < 0.4:
                    recent_unethical_prompt = entry["prompt"]
                    break
        else:
            print("Logger instance does not have usage_logs attribute.")


        similar_unethical_prompts = []
        if recent_unethical_prompt and hasattr(self.logger, 'find_similar_usage'):
            try:
                # Call find_similar_usage on the logger instance
                similar_usages = self.logger.find_similar_usage(recent_unethical_prompt, n=4)
                # Filter out the query prompt itself and keep only unethical ones with scores
                similar_unethical_prompts = [
                    u['original_log']['prompt'] for u in similar_usages
                    # Ensure 'ethical_score' exists and is not None in the original log
                    if u['original_log'].get('prompt') != recent_unethical_prompt and
                       u['original_log'].get("ethical_score") is not None and
                       u['original_log'].get("ethical_score", 1.0) < 0.4
                ][:3] # Limit to top 3 similar unethical prompts

            except Exception as e:
                print(f"Error finding similar usage for intervention context: {e}")


        # Use the LLMAbstractor to generate the intervention message
        suggestion_message = self.llm_abstractor.generate_intervention_suggestion(
            recent_unethical_prompt=recent_unethical_prompt,
            trust_level=self.trust_level,
            unethical_streak=self.unethical_streak,
            similar_unethical_prompts=similar_unethical_prompts
        )

        print(f"Intervention Suggestion: {suggestion_message}")
        return {"intervention_suggested": True, "message": suggestion_message}

"""## Helper Functions: AI Detection Model

### Subtask:
Include the necessary helper functions for AI detection.

**Reasoning**:
Define the `DesklibAIDetectionModel` class and the `predict_single_text` helper function required by the `EthicsModule` for AI detection. This addresses step 6 of the plan.
"""

import torch
import torch.nn as nn
from transformers import AutoConfig, AutoModel, AutoTokenizer, PreTrainedModel

# --- Helper Functions: AI Detection Model ---
class DesklibAIDetectionModel(PreTrainedModel):
    config_class = AutoConfig

    def __init__(self, config):
        super().__init__(config)
        # The base model architecture is loaded from the config
        self.model = AutoModel.from_config(config)
        self.classifier = nn.Linear(config.hidden_size, 1)
        self.init_weights()

    def forward(self, input_ids, attention_mask=None, labels=None):
        # Use the base model to get the last hidden state
        outputs = self.model(input_ids, attention_mask=attention_mask)
        last_hidden_state = outputs[0]

        # Pool the output (mean pooling)
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)
        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)
        pooled_output = sum_embeddings / sum_mask

        # Pass through the classifier
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            # Binary Cross-Entropy with Logits Loss for binary classification
            loss_fct = nn.BCEWithLogitsLoss()
            loss = loss_fct(logits.view(-1), labels.float())

        output = {"logits": logits}
        if loss is not None:
            output["loss"] = loss
        return output

def predict_single_text(text, model, tokenizer, device, max_len=768, threshold=0.5):
    """
    Predicts if a single text is AI-generated using the provided model.

    Args:
        text (str): The input text.
        model (PreTrainedModel): The AI detection model.
        tokenizer (AutoTokenizer): The tokenizer for the model.
        device (torch.device): The device to run inference on (cuda or cpu).
        max_len (int): Maximum sequence length.
        threshold (float): Probability threshold for classification.

    Returns:
        tuple: (probability, predicted_label, ethics_score_from_ai_detection)
               predicted_label is 1 for AI-generated, 0 otherwise.
               ethics_score_from_ai_detection is a simple score (1 - probability).
    """
    # Tokenize the input text
    encoded = tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=max_len,
        return_tensors='pt'
    )

    # Move inputs to the specified device
    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    # Set model to evaluation mode
    model.eval()

    # Perform inference without gradient calculation
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs["logits"]
        # Apply sigmoid to get probability
        probability = torch.sigmoid(logits).item()

    # Classify based on the threshold
    label = 1 if probability >= threshold else 0

    # Simple ethics score based on AI detection probability
    # Higher probability of AI means lower ethics score from this perspective
    ethics_score = 1 - probability

    # --- Debugging Prints ---
    print(f"Inside predict_single_text:")
    print(f"  Probability: {probability}")
    print(f"  Label: {label}")
    print(f"  Ethics Score: {ethics_score}")
    # --- End Debugging Prints ---

    return probability, label, ethics_score

def main():
    # --- Model and Tokenizer Directory ---
    model_directory = "desklib/ai-text-detector-v1.01"

    # --- Load tokenizer and model ---
    tokenizer = AutoTokenizer.from_pretrained(model_directory)
    model = DesklibAIDetectionModel.from_pretrained(model_directory)

    # --- Set up device ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # --- Example Input text ---
    text_ai = "AI detection refers to the process of identifying whether a given piece of content, such as text, images, or audio, has been generated by artificial intelligence. This is achieved using various machine learning techniques, including perplexity analysis, entropy measurements, linguistic pattern recognition, and neural network classifiers trained on human and AI-generated data. Advanced AI detection tools assess writing style, coherence, and statistical properties to determine the likelihood of AI involvement. These tools are widely used in academia, journalism, and content moderation to ensure originality, prevent misinformation, and maintain ethical standards. As AI-generated content becomes increasingly sophisticated, AI detection methods continue to evolve, integrating deep learning models and ensemble techniques for improved accuracy."
    text_human = "It is estimated that a major part of the content in the internet will be generated by AI / LLMs by 2025. This leads to a lot of misinformation and credibility related issues. That is why if is important to have accurate tools to identify if a content is AI generated or human written"

    # --- Run prediction ---
    probability, predicted_label,ethics_score = predict_single_text(text_ai, model, tokenizer, device)
    print(f"Probability of being AI generated: {probability:.4f}")
    print(f"Predicted label: {'AI Generated' if predicted_label == 1 else 'Not AI Generated'}")

    probability, predicted_label,ethics_score= predict_single_text(text_human, model, tokenizer, device)
    print(f"Probability of being AI generated: {probability:.4f}")
    print(f"Predicted label: {'AI Generated' if predicted_label == 1 else 'Not AI Generated'}")

if __name__ == "__main__":
    main()

"""## Demo: Multilingual Ethical Usage Tracking and Intervention

### Subtask:
Include a simplified demo or testing section that demonstrates multilingual capabilities and uses the abstracted LLM calls.

**Reasoning**:
Generate a code cell to initialize the LLMAbstractor, UsageLogger, and EthicsProfile, and then simulate interactions with multilingual prompts. This demonstrates the core functions and the system's ability to handle different languages using the abstracted LLM and Jina embeddings. This addresses step 7 of the plan.
"""

# Keep existing log files to demonstrate loading persistence.
print("Keeping existing log files for demo continuity.")

# --- OpenAI Client Initialization (Included in Demo) ---
# This code is repeated here to make the demo self-contained for client initialization.
# In a real application, you would typically initialize the client once at the beginning.
try:
    # Import the OpenAI library
    from openai import OpenAI
    # Used to securely store your API key - uncomment if using Colab Secrets
    from google.colab import userdata

    # Get your OpenAI API key securely
    openai_api_key_secure = userdata.get('OPENAI_API_KEY')
    openai_organization = userdata.get('OPENAI_ORGANIZATION')
    openai_project = userdata.get('OPENAI_PROJECT_ID')

    # Create the OpenAI client
    client = None # Initialize client to None
    if openai_api_key_secure:
         # Set project API key, organization, and project ID using the OpenAI class attributes
         OpenAI.api_key = openai_api_key_secure
         OpenAI.organization = openai_organization
         OpenAI.project = openai_project

         # Create the client instance
         client = OpenAI(api_key= OpenAI.api_key)


    # Check if client is initialized successfully
    if client:
        print("OpenAI client initialized successfully within the demo.")
    else:
        print("Failed to initialize OpenAI client within the demo.")

except Exception as e:
    print(f"Error during OpenAI client initialization in demo: {e}")
    client = None # Ensure client is None if initialization fails


# Ensure client is initialized before proceeding with the demo
if client:
    # Initialize the LLMAbstractor with the client
    try:
        llm_abstractor = LLMAbstractor(llm_client=client)
        print("LLMAbstractor initialized.")
    except NameError:
        print("LLMAbstractor class not defined. Please run the LLMAbstractor definition cell first.")
        llm_abstractor = None


    # Initialize the UsageLogger (this will load the Jina embedding model)
    if llm_abstractor: # Only proceed if LLMAbstractor is initialized
        try:
            logger = UsageLogger()
            print(f"\nInitialized UsageLogger. Logs: {len(logger.usage_logs)}, Embeddings: {len(logger.usage_embeddings)}")
        except NameError:
            print("UsageLogger class not defined. Please run the UsageLogger definition cell first.")
            logger = None

        # Initialize the EthicsProfile with the logger and llm_abstractor
        # Add checks for DesklibAIDetectionModel and predict_single_text
        if logger and 'DesklibAIDetectionModel' in globals() and 'predict_single_text' in globals(): # Only proceed if UsageLogger and AI detection helpers are initialized
            try:
                ethics_profile = EthicsProfile(logger=logger, llm_abstractor=llm_abstractor)
                print("EthicsProfile initialized.")
            except NameError:
                 # This NameError catch is less likely now with the explicit checks, but kept for robustness
                 print("EthicsProfile class not defined. Please run the EthicsProfile definition cell first.")
                 ethics_profile = None
        elif logger: # If logger is initialized but AI detection helpers are not
             print("\nSkipping EthicsProfile initialization as DesklibAIDetectionModel or predict_single_text are not defined. Please run their definition cells first.")
             ethics_profile = None
        else: # If logger is not initialized
            ethics_profile = None
            print("\nSkipping simulation as UsageLogger is not initialized.")


        if ethics_profile: # Only proceed if EthicsProfile is initialized
            # 5. Define a list of test prompts, including multilingual examples
            test_prompts = [
                {"prompt": "Can you help me brainstorm ideas for my thesis introduction?"}, # English - Ethical
                {"prompt": "Write my full literature review with citations."}, # English - Unethical
                {"prompt": "Summarize this article for me."}, # English - Ethical
                {"prompt": "write my entire essay on AI ethics"}, # English - Very Unethical
                {"prompt": "Generate keywords and an abstract for this title."}, # English - Neutral/Slightly Ethical
                {"prompt": "plagiarize this article"}, # English - Very Unethical
                {"prompt": "analyze the ethical implications of this AI model"}, # English - Ethical
                {"prompt": "Ayúdame a resumir este artículo."}, # Spanish - Ethical (Help me summarize this article)
                {"prompt": "Escribe mi ensayo completo sobre ética de la IA."}, # Spanish - Unethical (Write my full essay on ética de la IA)
                {"prompt": "Pouvez-vous m'aider à trouver des sources sur le biais d'apprentissage automatique ?"}, # French - Ethical (Can you help me find sources on machine learning bias?)
                {"prompt": "Écris-moi cet article en paraphrasant pour éviter le plagiat."}, # French - Unethical (Write me this article by paraphrasing to avoid plagiarism)
            ]

            print("\n--- Simulating Multilingual Usage and Updating Ethics Profile ---")
            # 6. Iterate through the test prompts and process each one
            for case in test_prompts:
                prompt = case['prompt']
                print(f"\nProcessing prompt: {prompt}")

                # - Call ethics_profile.check_ethical_usage()
                ethical_score, ethical_reason = ethics_profile.check_ethical_usage(prompt)
                print(f"Ethical Assessment: Score = {ethical_score}, Reason = {ethical_reason}")

                # - Call ethics_profile.detect_ai()
                is_ai, probability, ai_reason = ethics_profile.detect_ai(prompt)
                print(f"AI Detection: Is AI = {is_ai}, Probability = {probability:.4f}, Reason = {ai_reason}")


                # - Log usage with the obtained ethical score and reason
                # Use the score from the ethical check if available
                score_to_log = ethical_score if ethical_score is not None else None
                reason_to_log = ethical_reason if ethical_reason is not None else "Ethical assessment failed"

                logger.log_usage(prompt=prompt, ethical_score=score_to_log, ethical_reason=reason_to_log)
                print("Usage logged successfully.")

                # - Update ethics profile state
                ethics_profile.update_state(score_to_log)

                # - Show the current state
                ethics_profile.show_state()

                # - Suggest intervention if needed
                # This call previously caused an AttributeError, which should now be fixed.
                intervention_result = ethics_profile.suggest_intervention_standalone()
                print(f"Intervention Attempt: Suggested = {intervention_result['intervention_suggested']}")
                if intervention_result['intervention_suggested']:
                    print(f"Intervention Message: {intervention_result['message']}")


            print("\n--- Testing Multilingual UsageLogger.find_similar_usage ---")
            # 7. Test find_similar_usage with a multilingual query
            query_prompt_for_similarity = "ethical use of AI in academic research" # English query
            similar_usages = logger.find_similar_usage(query_prompt_for_similarity, n=3)
            print(f"Finding similar usage for query: '{query_prompt_for_similarity}' (English query)")
            if similar_usages:
                print("Similar usages found:")
                for i, sim in enumerate(similar_usages):
                    print(f"  {i+1}. Distance: {sim['distance']:.4f}, Prompt: '{sim['original_log']['prompt']}'")
                    if 'ethical_score' in sim['original_log']:
                        print(f"     Ethical Score: {sim['original_log']['ethical_score']}, Reason: {sim['original_log']['ethical_reason']}")
            else:
                print("No similar usages found.")

            query_prompt_for_similarity_es = "¿Cómo usar la IA éticamente en mi tesis?" # Spanish query (How to use AI ethically in my thesis?)
            similar_usages_es = logger.find_similar_usage(query_prompt_for_similarity_es, n=3)
            print(f"\nFinding similar usage for query: '{query_prompt_for_similarity_es}' (Spanish query)")
            if similar_usages_es:
                print("Similar usages found:")
                for i, sim in enumerate(similar_usages_es):
                    print(f"  {i+1}. Distance: {sim['distance']:.4f}, Prompt: '{sim['original_log']['prompt']}'")
                    if 'ethical_score' in sim['original_log']:
                        print(f"     Ethical Score: {sim['original_log']['ethical_score']}, Reason: {sim['original_log']['ethical_reason']}")
            else:
                print("No similar usages found.")

        # Added missing else for the ethics_profile check
        else:
            print("\nSkipping simulation as EthicsProfile is not initialized.")
    else: # Added missing else for the llm_abstractor check
        print("\nSkipping simulation as LLMAbstractor is not initialized.")
else:
    print("\nSkipping simulation as OpenAI client is not initialized.")
