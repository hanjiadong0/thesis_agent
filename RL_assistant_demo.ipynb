{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyND/G/FNbEesq+waL5ZWQR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanjiadong0/thesis_agent/blob/main/RL_assistant_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "777a22ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Provide the complete and simplified Python code in a single code block as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5acf73fe",
        "outputId": "579c4340-5b96-42f4-e2b9-79c865e0bd6d"
      },
      "source": [
        "!pip install openai stable-baselines3[extra] gymnasium langgraph pydantic numpy torch"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.70)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.6.0,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.5.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.74)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.7.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.11.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4YizPYgauhQE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0edb089",
        "outputId": "492c53f8-6c0e-4ff8-c6e1-318b45a5782b"
      },
      "source": [
        "# Combined Code for Thesis RL Agent (Ready for Merge)\n",
        "\n",
        "# --- Imports ---\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "import numpy as np\n",
        "from typing import Dict, Any, List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "import random\n",
        "import os\n",
        "from langgraph.graph import StateGraph, END\n",
        "from openai import OpenAI # Import OpenAI\n",
        "from google.colab import drive # Import drive\n",
        "import time # Import time for potential delay\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "# This needs to be done to access files in your Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    # Check if already mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    else:\n",
        "        print(\"Google Drive already mounted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Model persistence will not work.\")\n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Google Drive already mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f888bbd5"
      },
      "source": [
        "**Reasoning**:\n",
        "Analyze the `compute_reward` function to understand how it incentives state changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "223f61f7",
        "outputId": "e2761639-329f-4595-8f5f-06a57ba76e1f"
      },
      "source": [
        "# --- Core RL Definitions ---\n",
        "\n",
        "# 1. Define the state of the LangGraph using a Pydantic model\n",
        "class GraphState(BaseModel):\n",
        "    \"\"\"Represents the minimal state of the LangGraph for the Thesis Assistant.\"\"\"\n",
        "    user_input: str = \"\"\n",
        "    request_type: str = \"\"\n",
        "    thesis_stage: str = \"unknown\"\n",
        "    context: Dict[str, Any] = Field(default_factory=dict)\n",
        "    module_response: Any = None\n",
        "    rl_recommended_action: int = -1\n",
        "\n",
        "    # Core latent state - these represent the simplified state of the student's progress/well-being\n",
        "    personal_value: float = 0.5 # Represents personal growth/well-being (0.0 to 1.0)\n",
        "    quality: float = 0.5 # Represents thesis quality (0.0 to 1.0)\n",
        "    quantity: float = 0.0 # Represents thesis progress/quantity (0.0 to 1.0, 1.0 being completion)\n",
        "\n",
        "    # Relevant fields based on schemas.py and potentially derivable from ai_service.py context\n",
        "    thesis_field: str = \"Computer Science\"\n",
        "    procrastination_level: str = \"medium\"\n",
        "    writing_style: str = \"neutral\"\n",
        "    days_to_deadline: int = 0 # Calculated from thesis_deadline\n",
        "\n",
        "    # Simplified state variables that might be updated by \"simulate\" logic\n",
        "    emotion_state: float = 0.5\n",
        "    deadline_ratio: float = 0.0 # Kept: Derived from days_to_deadline/initial deadline\n",
        "\n",
        "    # Add a key for the final formatted response\n",
        "    formatted_response: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "\n",
        "# 2. Define a mock Settings class based on config.py\n",
        "class MockSettings:\n",
        "    \"\"\"Mock Settings class mimicking config.py for AI model name access.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.AI_MODEL: str = \"gpt-4o-mini\" # Using a suitable model for this task\n",
        "\n",
        "# Instantiate the mock settings\n",
        "settings = MockSettings()\n",
        "\n",
        "\n",
        "# 3. Define the encode_minimal_state function\n",
        "RAW_STATE_DIM = 7 # Reduced dimension based on simplified GraphState\n",
        "observation_space_raw = gym.spaces.Box(low=0.0, high=1.0, shape=(RAW_STATE_DIM,), dtype=np.float32)\n",
        "\n",
        "def encode_minimal_state(state: GraphState) -> List[float]:\n",
        "    \"\"\"\n",
        "    Encode a minimal state vector for the PPO agent using simplified GraphState.\n",
        "    Returns a list of RAW_STATE_DIM float values.\n",
        "    \"\"\"\n",
        "    state_vector = []\n",
        "\n",
        "    # Features from simplified GraphState\n",
        "    state_vector.append(state.personal_value) # 0-1\n",
        "    state_vector.append(state.quality) # 0-1\n",
        "    state_vector.append(state.quantity) # 0-1\n",
        "    state_vector.append(state.emotion_state) # 0-1\n",
        "    state_vector.append(state.deadline_ratio) # 0-1\n",
        "\n",
        "    # Encode categorical features (simple numerical mapping)\n",
        "    thesis_field_map = {\"Computer Science\": 0.0, \"Psychology\": 0.5, \"History\": 1.0}\n",
        "    procrastination_map = {\"low\": 0.0, \"medium\": 0.5, \"high\": 1.0}\n",
        "\n",
        "    state_vector.append(thesis_field_map.get(state.thesis_field, 0.5))\n",
        "    state_vector.append(procrastination_map.get(state.procrastination_level, 0.5))\n",
        "\n",
        "    # Ensure the vector matches RAW_STATE_DIM\n",
        "    while len(state_vector) < RAW_STATE_DIM:\n",
        "        state_vector.append(0.5) # Pad with a neutral value if needed\n",
        "    while len(state_vector) > RAW_STATE_DIM:\n",
        "        state_vector.pop() # Trim if needed\n",
        "\n",
        "    return state_vector\n",
        "\n",
        "\n",
        "LATENT_STATE_DIM = 3 # The target dimension for the encoder output\n",
        "\n",
        "# Discrete action space for the 5 advisor actions:\n",
        "ACTION_LABELS = [\"Plan next steps\", \"Suggest writing\", \"Recommend reflection\", \"Offer encouragement\", \"Advisor feedback reminder\"]\n",
        "action_space = gym.spaces.Discrete(len(ACTION_LABELS))\n",
        "\n",
        "\n",
        "# 4. Define the Custom State Encoder Layer (PyTorch Module)\n",
        "class StateEncoder(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    Custom feature extractor (encoder) to map the raw state to the 3D latent state.\n",
        "    \"\"\"\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = LATENT_STATE_DIM, hidden_dim: int = 64):\n",
        "        super(StateEncoder, self).__init__(observation_space, features_dim)\n",
        "        self.shared = nn.Linear(self._observation_space.shape[0], hidden_dim)\n",
        "        self.latent = nn.Linear(hidden_dim, features_dim)\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        observations = observations.float()\n",
        "        h = torch.relu(self.shared(observations))\n",
        "        return torch.sigmoid(self.latent(h))\n",
        "\n",
        "\n",
        "# 5. Define the Reward Function\n",
        "def compute_reward(state: GraphState, previous_state: GraphState) -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on the current and previous state.\n",
        "    Reward is shaped by personal_value, quality, and quantity, with penalties.\n",
        "    \"\"\"\n",
        "    personal_value = state.personal_value\n",
        "    quality = state.quality\n",
        "    quantity = state.quantity\n",
        "    deadline_ratio = state.deadline_ratio\n",
        "    time_left = 1.0 - deadline_ratio\n",
        "\n",
        "    momentum_bonus = max(0.0, quantity - previous_state.quantity) * 0.5\n",
        "\n",
        "    reward = (\n",
        "        + 1.0 * (personal_value - 0.5)\n",
        "        + 0.3 * quality\n",
        "        + 0.3 * quantity\n",
        "        - 0.2 * abs(quality - quantity)\n",
        "        - 0.4 * (1 - time_left)\n",
        "        + momentum_bonus\n",
        "    )\n",
        "    return reward\n",
        "\n",
        "\n",
        "# 6. Define a Minimal RL Environment (Gym-style)\n",
        "class ThesisEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    A minimal, simulated Gym-style RL environment for the Thesis Assistant.\n",
        "    Simulates student responses and updates the raw state and GraphState.\n",
        "    Accepts an optional llm_client.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm_client=None): # Accept optional llm_client\n",
        "        super().__init__()\n",
        "        self.observation_space = observation_space_raw\n",
        "        self.action_space = action_space\n",
        "        self._state = GraphState()\n",
        "        self._previous_state = GraphState()\n",
        "        self._raw_state = np.array(encode_minimal_state(self._state), dtype=np.float32)\n",
        "        self._timestep = 0\n",
        "        self._max_timesteps = 100\n",
        "        self._initial_days_to_deadline = 0\n",
        "        self.llm_client = llm_client # Store the provided LLM client\n",
        "\n",
        "    def step(self, action):\n",
        "        self._timestep += 1\n",
        "        self._previous_state = self._state.model_copy(deep=True)\n",
        "\n",
        "        # Simulate changes to graph state based on action and random chance (simplified)\n",
        "        personal_value = self._state.personal_value\n",
        "        quality = self._state.quality\n",
        "        quantity = self._state.quantity\n",
        "        emotion_state = self._state.emotion_state\n",
        "        days_to_deadline = self._state.days_to_deadline\n",
        "\n",
        "        if action == 0: # Plan\n",
        "             quantity = min(quantity + np.random.uniform(0.01, 0.05), 1.0)\n",
        "             personal_value = min(personal_value + np.random.uniform(0.01, 0.03), 1.0)\n",
        "        elif action == 1: # Write\n",
        "             quantity = min(quantity + np.random.uniform(0.02, 0.1), 1.0)\n",
        "             quality = min(quality + np.random.uniform(0.00, 0.02), 1.0)\n",
        "        elif action == 2: # Reflect\n",
        "             quality = min(quality + np.random.uniform(0.01, 0.05), 1.0)\n",
        "             personal_value = min(personal_value + np.random.uniform(0.01, 0.05), 1.0)\n",
        "        elif action == 3: # Encourage\n",
        "             emotion_state = max(emotion_state - np.random.uniform(0.1, 0.3), 0.0)\n",
        "             personal_value = min(personal_value + np.random.uniform(0.02, 0.05), 1.0)\n",
        "        elif action == 4: # Feedback Reminder\n",
        "             quality = min(quality + np.random.uniform(0.00, 0.01), 1.0)\n",
        "             emotion_state = np.clip(emotion_state + np.random.normal(0, 0.03), 0.0, 1.0)\n",
        "        else:\n",
        "            print(f\"Warning: Unknown action index {action}. No specific simulation rule applied.\")\n",
        "\n",
        "        # Simulate time passing and update days_to_deadline and deadline_ratio\n",
        "        days_passed = random.randint(1, 7)\n",
        "        days_to_deadline = max(0, days_to_deadline - days_passed)\n",
        "        initial_days = self._state.context.get('initial_days_to_deadline', 1)\n",
        "        deadline_ratio = 1.0 - (days_to_deadline / initial_days)\n",
        "        deadline_ratio = np.clip(deadline_ratio, 0.0, 1.0)\n",
        "\n",
        "        # Update the state\n",
        "        self._state.personal_value = personal_value\n",
        "        self._state.quality = quality\n",
        "        self._state.quantity = quantity\n",
        "        self._state.emotion_state = emotion_state\n",
        "        self._state.days_to_deadline = days_to_deadline\n",
        "        self._state.deadline_ratio = deadline_ratio\n",
        "\n",
        "        # Re-encode raw state\n",
        "        self._raw_state = np.array(encode_minimal_state(self._state), dtype=np.float32)\n",
        "\n",
        "        reward = compute_reward(self._state, self._previous_state)\n",
        "\n",
        "        # Exit conditions based on simplified state\n",
        "        done = self._state.quantity >= 1.0 or self._state.personal_value >= 0.95 or self._timestep >= self._max_timesteps or self._state.days_to_deadline <= 0\n",
        "\n",
        "        info = {\n",
        "            \"current_graph_state\": self._state.model_dump(),\n",
        "            \"previous_graph_state\": self._previous_state.model_dump(),\n",
        "            \"timestep\": self._timestep\n",
        "            }\n",
        "        # Include the llm_client in the info dictionary so the node can access it\n",
        "        info['llm_client'] = self.llm_client\n",
        "\n",
        "\n",
        "        return self._raw_state, reward, done, False, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        initial_days = random.randint(100, 500)\n",
        "        self._initial_days_to_deadline = initial_days\n",
        "\n",
        "        self._state = GraphState(\n",
        "            personal_value=np.random.uniform(0.4, 0.6),\n",
        "            quality=np.random.uniform(0.4, 0.6),\n",
        "            quantity=0.0,\n",
        "            emotion_state=np.random.uniform(0.4, 0.6),\n",
        "            deadline_ratio=0.0,\n",
        "            thesis_field=random.choice([\"Computer Science\", \"Psychology\", \"History\"]),\n",
        "            procrastination_level=random.choice([\"low\", \"medium\", \"high\"]),\n",
        "            writing_style=random.choice([\"concise\", \"descriptive\", \"neutral\"]),\n",
        "            days_to_deadline=initial_days\n",
        "        )\n",
        "\n",
        "        self._previous_state = self._state.model_copy(deep=True)\n",
        "        self._raw_state = np.array(encode_minimal_state(self._state), dtype=np.float32)\n",
        "        self._timestep = 0\n",
        "        info = {\"initial_graph_state\": self._state.model_dump()}\n",
        "        info['initial_graph_state']['context']['initial_days_to_deadline'] = self._initial_days_to_deadline\n",
        "        # Also include the llm_client in the initial info\n",
        "        info['llm_client'] = self.llm_client\n",
        "\n",
        "\n",
        "        return self._raw_state, info\n",
        "\n",
        "    def render(self):\n",
        "        print(\"\\nCurrent State (Simulated Environment):\")\n",
        "        print(f\"  Raw State (partial): {self._raw_state[:5]}...\")\n",
        "        print(f\"  Latent State: PV={self._state.personal_value:.2f}, QL={self._state.quality:.2f}, QT={self._state.quantity:.2f}\")\n",
        "        print(f\"  Emotion: {self._state.emotion_state:.2f}, Deadline: {self._state.deadline_ratio:.2f}, Days Left: {self._state.days_to_deadline}\")\n",
        "        print(f\"  Thesis Context: Field={self._state.thesis_field}, Procrastination={self._state.procrastination_level}, Writing Style={self._state.writing_style}\")\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# 7. Define MODEL_PATH and policy_kwargs\n",
        "# Define the base path for saving/loading from Google Drive\n",
        "DRIVE_MODEL_DIR = \"/content/drive/MyDrive/Colab_Thesis_Models\"\n",
        "# IMPORTANT: Define the filename WITH the .zip extension\n",
        "MODEL_FILENAME = \"thesis_policy.pt.zip\" # Corrected filename\n",
        "MODEL_PATH = os.path.join(DRIVE_MODEL_DIR, MODEL_FILENAME)\n",
        "\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=StateEncoder,\n",
        "    features_extractor_kwargs=dict(features_dim=LATENT_STATE_DIM, hidden_dim=64),\n",
        "    net_arch=dict(pi=[64, 64], vf=[64, 64])\n",
        ")\n",
        "\n",
        "# --- Mock OpenAI Client Definition ---\n",
        "class MockOpenAIClient:\n",
        "    \"\"\"A simple mock client to simulate OpenAI API calls during training.\"\"\"\n",
        "    def chat(self):\n",
        "        class Completions:\n",
        "            def create(self, model, messages, max_tokens=None, temperature=None):\n",
        "                # Simulate a simple, consistent response for training\n",
        "                class MockChoice:\n",
        "                    class MockMessage:\n",
        "                        content = \"Mock advisor suggestion for training.\"\n",
        "                    message = MockMessage()\n",
        "                class MockResponse:\n",
        "                    choices = [MockChoice()]\n",
        "                return MockResponse()\n",
        "        completions = Completions()\n",
        "        return completions\n",
        "\n",
        "# Define a function to create the environment, injecting a specific LLM client\n",
        "def make_env_with_client(llm_client_instance):\n",
        "    def _init():\n",
        "        return ThesisEnv(llm_client=llm_client_instance)\n",
        "    return _init\n",
        "# --- End Mock OpenAI Client Definition ---\n",
        "\n",
        "\n",
        "# 8. Define the LangGraph Node Functions\n",
        "# Modify policy_node to use a pre-loaded model passed via config\n",
        "def policy_node(state: GraphState, config: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Uses a pre-loaded PPO model from config to determine the action.\n",
        "    Returns a dictionary update for the state with the recommended action index.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Executing Policy Node ---\")\n",
        "\n",
        "    # Attempt to get the pre-loaded model from the config dictionary\n",
        "    # Access it via config.get(\"configurable\", {}).get(\"ppo_model\")\n",
        "    # We now expect the model to be passed here after being loaded once before the stream\n",
        "    model = config.get(\"configurable\", {}).get(\"ppo_model\")\n",
        "\n",
        "    if model is None:\n",
        "        print(\"Error: PPO model not available in config. Falling back to random action.\")\n",
        "        # We might get here if the model failed to load BEFORE the stream started.\n",
        "        # In this case, we can optionally try to load it again or just fallback.\n",
        "        # Let's stick to falling back here if it wasn't successfully loaded before.\n",
        "        recommended_action = random.randint(0, len(ACTION_LABELS) - 1) # Fallback to random action\n",
        "    else:\n",
        "        try:\n",
        "            print(\"PPO model found in config. Proceeding with prediction.\") # Debug print\n",
        "            raw_state_vector = np.array(encode_minimal_state(state), dtype=np.float32)\n",
        "\n",
        "            if raw_state_vector.shape[0] != RAW_STATE_DIM:\n",
        "                 print(f\"Warning: Raw state dimension mismatch. Expected {RAW_STATE_DIM}, got {raw_state_vector.shape[0]}. Adjusting.\")\n",
        "                 if raw_state_vector.shape[0] > RAW_STATE_DIM:\n",
        "                      raw_state_vector = raw_state_vector[:RAW_STATE_DIM]\n",
        "                 else:\n",
        "                      raw_state_vector = np.pad(raw_state_vector, (0, RAW_STATE_DIM - raw_state_vector.shape[0]), 'constant', constant_values=0.5)\n",
        "\n",
        "\n",
        "            # Reshape observation for prediction\n",
        "            obs_batch = np.array([raw_state_vector], dtype=np.float32)\n",
        "            action, _states = model.predict(obs_batch, deterministic=True)\n",
        "            recommended_action = int(action[0])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model prediction: {e}. Falling back to random action.\")\n",
        "            recommended_action = random.randint(0, len(ACTION_LABELS) - 1) # Fallback to random action\n",
        "\n",
        "\n",
        "    print(f\"Policy Node Output: Recommended Action Index = {recommended_action} ({ACTION_LABELS[recommended_action]})\")\n",
        "\n",
        "    updated_context = state.context.copy()\n",
        "    # Don't store raw_state in context unless needed elsewhere\n",
        "    # updated_context[\"raw_state\"] = raw_state_vector.tolist()\n",
        "\n",
        "    # Pass the llm_client through the context if it exists\n",
        "    llm_client_instance = state.context.get(\"llm_client\")\n",
        "    if llm_client_instance is not None:\n",
        "        updated_context['llm_client'] = llm_client_instance\n",
        "\n",
        "\n",
        "    return {\"rl_recommended_action\": recommended_action, \"context\": updated_context}\n",
        "\n",
        "\n",
        "# llm_node uses the client from the environment instance (passed via context)\n",
        "def llm_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Uses the LLM client from the environment instance (passed via context)\n",
        "    or falls back to the global client to generate an advisor suggestion.\n",
        "    Returns a dictionary update for the state with the module response.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Executing LLM Abstraction Node (using OpenAI) ---\")\n",
        "    action_index = state.rl_recommended_action\n",
        "\n",
        "    if action_index is None or action_index < 0 or action_index >= len(ACTION_LABELS):\n",
        "        print(f\"Warning: Invalid action index received: {action_index}. Defaulting to 'Plan next steps'.\")\n",
        "        action_index = 0\n",
        "\n",
        "    recommended_action_label = ACTION_LABELS[action_index]\n",
        "\n",
        "    # Attempt to get the LLM client from state context\n",
        "    llm_client_instance = state.context.get(\"llm_client\")\n",
        "\n",
        "    if llm_client_instance is None:\n",
        "        # If no client is in context, use the global client (for inference)\n",
        "        global client\n",
        "        llm_client_instance = client\n",
        "        print(\"Using global OpenAI client for LLM node.\")\n",
        "    else:\n",
        "        print(\"Using client from state context for LLM node (likely mock).\")\n",
        "\n",
        "    if llm_client_instance is None or not hasattr(llm_client_instance, 'chat'):\n",
        "         print(\"Error: LLM client is not initialized or not accessible.\")\n",
        "         advisor_suggestion = f\"Advisor Suggestion (Fallback): Consider action '{recommended_action_label}'. (Error: LLM client not initialized)\"\n",
        "         return {\"module_response\": advisor_suggestion}\n",
        "\n",
        "    # Construct the prompt for the OpenAI API\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI thesis advisor. Your goal is to provide concise, helpful suggestions to a student based on their current progress and a recommended action.\n",
        "\n",
        "    Student Context:\n",
        "    - Thesis Field: {state.thesis_field}\n",
        "    - Procrastination Level: {state.procrastination_level}\n",
        "    - Writing Style: {state.writing_style}\n",
        "    - Estimated Days to Deadline: {state.days_to_deadline}\n",
        "    - Current Progress (Quantity): {state.quantity:.1f}/1.0\n",
        "    - Estimated Quality: {state.quality:.1f}/1.0\n",
        "    - Personal Well-being: {state.personal_value:.1f}/1.0\n",
        "    - Emotional State: {state.emotion_state:.1f}/1.0\n",
        "\n",
        "    Recommended Action (from AI Agent): \"{recommended_action_label}\"\n",
        "\n",
        "    Based on the recommended action and the student's context, provide a brief, encouraging, and actionable advisor suggestion (1-3 sentences). Tailor the suggestion to the student's context if relevant.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm_client_instance.chat.completions.create(\n",
        "            model=settings.AI_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an AI thesis advisor.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "\n",
        "        if response.choices and response.choices[0].message.content:\n",
        "            advisor_suggestion = response.choices[0].message.content.strip()\n",
        "        else:\n",
        "            advisor_suggestion = f\"Advisor Suggestion: Consider action '{recommended_action_label}'. (No specific suggestion generated by LLM.)\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM API call: {e}\")\n",
        "        advisor_suggestion = f\"Advisor Suggestion (API Error): Consider action '{recommended_action_label}'. (API Error: {e})\"\n",
        "\n",
        "    return {\"module_response\": advisor_suggestion}\n",
        "\n",
        "\n",
        "def core_update_node(state: GraphState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Updates the core latent state and increments step_count.\n",
        "    Returns a dictionary with the updates to the state.\n",
        "    Assumes update_mode is \"simulate\".\n",
        "    Simplified state updates.\n",
        "    \"\"\"\n",
        "    update_mode = \"simulate\"\n",
        "    print(f\"\\n--- Executing Core Update Node in '{update_mode}' mode ---\")\n",
        "\n",
        "    updated_context = state.context.copy()\n",
        "    step_count = updated_context.get('step_count', 0) + 1\n",
        "    updated_context['step_count'] = step_count\n",
        "\n",
        "    # Get current state values\n",
        "    personal_value = state.personal_value\n",
        "    quality = state.quality\n",
        "    quantity = state.quantity\n",
        "    emotion_state = state.emotion_state\n",
        "    deadline_ratio = state.deadline_ratio\n",
        "    days_to_deadline = state.days_to_deadline\n",
        "    action_index = state.rl_recommended_action\n",
        "\n",
        "    if update_mode == \"simulate\":\n",
        "        # Simulate state changes based on action\n",
        "        if action_index == 0: # Plan\n",
        "             quantity = min(quantity + np.random.uniform(0.01, 0.05), 1.0)\n",
        "             personal_value = min(personal_value + np.random.uniform(0.01, 0.03), 1.0)\n",
        "        elif action_index == 1: # Write\n",
        "             quantity = min(quantity + np.random.uniform(0.02, 0.1), 1.0)\n",
        "             quality = min(quality + np.random.uniform(0.00, 0.02), 1.0)\n",
        "        elif action_index == 2: # Reflect\n",
        "             quality = min(quality + np.random.uniform(0.01, 0.05), 1.0)\n",
        "             personal_value = min(personal_value + np.random.uniform(0.01, 0.05), 1.0)\n",
        "        elif action_index == 3: # Encourage\n",
        "             emotion_state = max(emotion_state - np.random.uniform(0.1, 0.3), 0.0)\n",
        "             personal_value = min(personal_value + np.random.uniform(0.02, 0.05), 1.0)\n",
        "        elif action_index == 4: # Feedback Reminder\n",
        "             quality = min(quality + np.random.uniform(0.00, 0.01), 1.0)\n",
        "             emotion_state = np.clip(emotion_state + np.random.normal(0, 0.03), 0.0, 1.0)\n",
        "        else:\n",
        "            print(f\"Warning: Unknown action index {action_index}. No specific simulation rule applied.\")\n",
        "\n",
        "        # Simulate time passing and update days_to_deadline and deadline_ratio\n",
        "        days_passed = random.randint(1, 7)\n",
        "        days_to_deadline = max(0, days_to_deadline - days_passed)\n",
        "        initial_days = state.context.get('initial_days_to_deadline', 1)\n",
        "        deadline_ratio = 1.0 - (days_to_deadline / initial_days)\n",
        "        deadline_ratio = np.clip(deadline_ratio, 0.0, 1.0)\n",
        "\n",
        "    print(f\"State after update: PV={personal_value:.2f}, QL={quality:.2f}, QT={quantity:.2f}, Steps={step_count}, Deadline={deadline_ratio:.2f}, Days Left={days_to_deadline}\")\n",
        "\n",
        "    # Pass the llm_client through the context if it exists\n",
        "    llm_client_instance = state.context.get(\"llm_client\")\n",
        "    if llm_client_instance is not None:\n",
        "        updated_context['llm_client'] = llm_client_instance\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"personal_value\": personal_value,\n",
        "        \"quality\": quality,\n",
        "        \"quantity\": quantity,\n",
        "        \"emotion_state\": emotion_state,\n",
        "        \"deadline_ratio\": deadline_ratio,\n",
        "        \"days_to_deadline\": days_to_deadline,\n",
        "        \"context\": updated_context,\n",
        "        \"module_response\": state.module_response # Pass the LLM response through\n",
        "    }\n",
        "\n",
        "\n",
        "def check_exit_node(state: GraphState) -> str:\n",
        "    \"\"\"\n",
        "    Checks if the simulation should exit based on state conditions.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Executing Check Exit Node ---\")\n",
        "    step_count = state.context.get(\"step_count\", 0)\n",
        "    personal_value = state.personal_value\n",
        "    quantity = state.quantity\n",
        "    days_to_deadline = state.days_to_deadline\n",
        "\n",
        "    print(f\"Checking exit conditions: Step Count = {step_count}, Personal Value = {personal_value:.2f}, Quantity = {quantity:.2f}, Days Left = {days_to_deadline}\")\n",
        "\n",
        "    if step_count >= 10: # Reduced steps for a shorter demo\n",
        "        print(\"Exit condition met: Maximum steps reached.\")\n",
        "        return \"end\"\n",
        "    elif personal_value >= 0.95:\n",
        "        print(\"Exit condition met: Personal value goal reached.\")\n",
        "        return \"end\"\n",
        "    elif quantity >= 1.0:\n",
        "        print(\"Exit condition met: Quantity goal reached (Thesis complete).\")\n",
        "        return \"end\"\n",
        "    elif days_to_deadline <= 0:\n",
        "        print(\"Exit condition met: Deadline reached.\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"Exit conditions not met. Continuing.\")\n",
        "        return \"policy_node\"\n",
        "\n",
        "\n",
        "# 9. Assemble the LangGraph\n",
        "# Graph assembly remains the same, the model is passed via config\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"policy_node\", policy_node)\n",
        "workflow.add_node(\"llm_node\", llm_node)\n",
        "workflow.add_node(\"core_update_node\", core_update_node)\n",
        "workflow.set_entry_point(\"policy_node\")\n",
        "workflow.add_edge(\"policy_node\", \"llm_node\")\n",
        "workflow.add_edge(\"llm_node\", \"core_update_node\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"core_update_node\",\n",
        "    check_exit_node,\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"policy_node\": \"policy_node\"\n",
        "    }\n",
        ")\n",
        "app = workflow.compile()\n",
        "\n",
        "\n",
        "# --- PPO Model Training (using Mock Client) ---\n",
        "print(\"\\n--- Starting PPO Model Training (using Mock Client) ---\")\n",
        "\n",
        "# Define training parameters\n",
        "total_timesteps = 100000  # You can adjust this\n",
        "learning_rate = 3e-4\n",
        "gamma = 0.99\n",
        "gae_lambda = 0.95\n",
        "clip_range = 0.2\n",
        "\n",
        "# Instantiate the mock client for training\n",
        "mock_client = MockOpenAIClient()\n",
        "\n",
        "# Create the training environment using the mock client\n",
        "train_env = DummyVecEnv([make_env_with_client(mock_client)])\n",
        "\n",
        "# Define MODEL_PATH (Google Drive)\n",
        "DRIVE_MODEL_DIR = \"/content/drive/MyDrive/Colab_Thesis_Models\"\n",
        "# IMPORTANT: Define the filename WITH the .zip extension\n",
        "MODEL_FILENAME = \"thesis_policy.pt.zip\" # Corrected filename\n",
        "MODEL_PATH = os.path.join(DRIVE_MODEL_DIR, MODEL_FILENAME)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
        "print(f\"Model directory ensured at: {DRIVE_MODEL_DIR}\")\n",
        "\n",
        "\n",
        "# Initialize or load the PPO model for TRAINING from Drive\n",
        "# This block handles continued learning\n",
        "model = None\n",
        "print(f\"Training block checking for model at: {MODEL_PATH}\") # Debug print\n",
        "# Check for the existence of the file with the .zip extension\n",
        "if os.path.exists(MODEL_PATH): # MODEL_PATH now includes .zip\n",
        "    print(f\"Model file found at {MODEL_PATH} for training. Attempting to load.\") # Debug print\n",
        "    try:\n",
        "        temp_env_instance = ThesisEnv(llm_client=mock_client) # Create temp env with mock client for loading\n",
        "        temp_env = DummyVecEnv([lambda: temp_env_instance])\n",
        "        # Load using the path that includes .zip\n",
        "        model = PPO.load(MODEL_PATH, env=temp_env, device=\"auto\", custom_objects={\"StateEncoder\": StateEncoder})\n",
        "        print(f\"Existing model loaded from {MODEL_PATH}. Continuing training.\")\n",
        "        temp_env.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading existing model for training from {MODEL_PATH}: {e}. Initializing a new model.\")\n",
        "        model = PPO(\n",
        "            \"MlpPolicy\",\n",
        "            train_env,\n",
        "            learning_rate=learning_rate,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            clip_range=clip_range,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=1,\n",
        "            device=\"auto\"\n",
        "        )\n",
        "else:\n",
        "    print(f\"No existing model found at {MODEL_PATH}. Initializing a new model.\")\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        train_env,\n",
        "        learning_rate=learning_rate,\n",
        "        gamma=gamma,\n",
        "        gae_lambda=gae_lambda,\n",
        "        clip_range=clip_range,\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=1,\n",
        "        device=\"auto\"\n",
        "    )\n",
        "\n",
        "# Train the PPO model\n",
        "print(f\"Training for {total_timesteps} timesteps...\")\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Save the trained model to Drive using the path that includes .zip\n",
        "model.save(MODEL_PATH) # model.save will overwrite if exists\n",
        "print(f\"Trained model saved to {MODEL_PATH}\")\n",
        "\n",
        "# Add a small delay after saving to allow file system to sync, especially with Drive\n",
        "# This is a potential workaround for timing issues in single-cell execution\n",
        "print(\"Adding a small delay after saving...\")\n",
        "time.sleep(5) # Wait for 5 seconds\n",
        "print(\"Delay finished.\")\n",
        "\n",
        "# Close the training environment\n",
        "train_env.close()\n",
        "print(\"Training environment closed.\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting PPO Model Training (using Mock Client) ---\n",
            "Model directory ensured at: /content/drive/MyDrive/Colab_Thesis_Models\n",
            "Training block checking for model at: /content/drive/MyDrive/Colab_Thesis_Models/thesis_policy.pt.zip\n",
            "Model file found at /content/drive/MyDrive/Colab_Thesis_Models/thesis_policy.pt.zip for training. Attempting to load.\n",
            "Existing model loaded from /content/drive/MyDrive/Colab_Thesis_Models/thesis_policy.pt.zip. Continuing training.\n",
            "Training for 100000 timesteps...\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 940  |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 2    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 566         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 7           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004518356 |\n",
            "|    clip_fraction        | 0.0433      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.941      |\n",
            "|    explained_variance   | 0.52        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.68        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.00283    |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 549         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002986465 |\n",
            "|    clip_fraction        | 0.0318      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.874      |\n",
            "|    explained_variance   | 0.503       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.78        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | 0.000145    |\n",
            "|    value_loss           | 12.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 550         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003220468 |\n",
            "|    clip_fraction        | 0.03        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.945      |\n",
            "|    explained_variance   | 0.529       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.34        |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | 0.00057     |\n",
            "|    value_loss           | 15.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 548          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036602146 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.885       |\n",
            "|    explained_variance   | 0.378        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.54         |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.000635    |\n",
            "|    value_loss           | 16.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 550          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070103044 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.852       |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.58         |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | 0.00265      |\n",
            "|    value_loss           | 12.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 550          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053698965 |\n",
            "|    clip_fraction        | 0.046        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.963       |\n",
            "|    explained_variance   | 0.274        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.78         |\n",
            "|    n_updates            | 650          |\n",
            "|    policy_gradient_loss | 0.00318      |\n",
            "|    value_loss           | 22.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 550          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035918946 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.929       |\n",
            "|    explained_variance   | 0.531        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.43         |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | -0.00087     |\n",
            "|    value_loss           | 14.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 549          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062638717 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.954       |\n",
            "|    explained_variance   | 0.353        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.14         |\n",
            "|    n_updates            | 670          |\n",
            "|    policy_gradient_loss | -0.00154     |\n",
            "|    value_loss           | 21.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 550          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058070035 |\n",
            "|    clip_fraction        | 0.0437       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.8         |\n",
            "|    explained_variance   | 0.579        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.53         |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | 0.00838      |\n",
            "|    value_loss           | 11.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 551          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060566864 |\n",
            "|    clip_fraction        | 0.0658       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.876       |\n",
            "|    explained_variance   | 0.465        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.9          |\n",
            "|    n_updates            | 690          |\n",
            "|    policy_gradient_loss | 0.00325      |\n",
            "|    value_loss           | 16.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 549          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 24576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043700486 |\n",
            "|    clip_fraction        | 0.0292       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.9         |\n",
            "|    explained_variance   | 0.435        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.17         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.00154     |\n",
            "|    value_loss           | 16.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 550          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 48           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044620843 |\n",
            "|    clip_fraction        | 0.0434       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.892       |\n",
            "|    explained_variance   | 0.543        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.03         |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | 0.00157      |\n",
            "|    value_loss           | 16.9         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 551        |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 51         |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00398754 |\n",
            "|    clip_fraction        | 0.0309     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.892     |\n",
            "|    explained_variance   | 0.516      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 10.1       |\n",
            "|    n_updates            | 720        |\n",
            "|    policy_gradient_loss | -0.000265  |\n",
            "|    value_loss           | 17.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 552         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 55          |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004277993 |\n",
            "|    clip_fraction        | 0.0315      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.855      |\n",
            "|    explained_variance   | 0.486       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.52        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | 0.000612    |\n",
            "|    value_loss           | 20.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 552         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 59          |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007477736 |\n",
            "|    clip_fraction        | 0.0671      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.856      |\n",
            "|    explained_variance   | 0.493       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.68        |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | 0.00891     |\n",
            "|    value_loss           | 16.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 552          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 62           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0073873485 |\n",
            "|    clip_fraction        | 0.0735       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.911       |\n",
            "|    explained_variance   | 0.342        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.37         |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.000905    |\n",
            "|    value_loss           | 19.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 551         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 66          |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006770733 |\n",
            "|    clip_fraction        | 0.075       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.919      |\n",
            "|    explained_variance   | 0.349       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.89        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.00235     |\n",
            "|    value_loss           | 18.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 552        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 70         |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00780946 |\n",
            "|    clip_fraction        | 0.0762     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.901     |\n",
            "|    explained_variance   | 0.512      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.58       |\n",
            "|    n_updates            | 770        |\n",
            "|    policy_gradient_loss | -0.000702  |\n",
            "|    value_loss           | 13.4       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 552          |\n",
            "|    iterations           | 20           |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041478127 |\n",
            "|    clip_fraction        | 0.047        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.885       |\n",
            "|    explained_variance   | 0.385        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.19         |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | 0.00168      |\n",
            "|    value_loss           | 16.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 77          |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006752942 |\n",
            "|    clip_fraction        | 0.0719      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.851      |\n",
            "|    explained_variance   | 0.48        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.28        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | 0.00216     |\n",
            "|    value_loss           | 17.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 81           |\n",
            "|    total_timesteps      | 45056        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038556587 |\n",
            "|    clip_fraction        | 0.041        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.779       |\n",
            "|    explained_variance   | 0.492        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.37         |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | 0.00242      |\n",
            "|    value_loss           | 15.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 85           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039766664 |\n",
            "|    clip_fraction        | 0.0436       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.786       |\n",
            "|    explained_variance   | 0.448        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.32         |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | 0.00516      |\n",
            "|    value_loss           | 15.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 88           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059950096 |\n",
            "|    clip_fraction        | 0.0651       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.718       |\n",
            "|    explained_variance   | 0.346        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.6         |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | 0.00184      |\n",
            "|    value_loss           | 19           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039697126 |\n",
            "|    clip_fraction        | 0.0379       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.736       |\n",
            "|    explained_variance   | 0.637        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.07         |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.00131     |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 555         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 95          |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008461285 |\n",
            "|    clip_fraction        | 0.0946      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.744      |\n",
            "|    explained_variance   | 0.208       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.17        |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.00466     |\n",
            "|    value_loss           | 17.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 99           |\n",
            "|    total_timesteps      | 55296        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050440375 |\n",
            "|    clip_fraction        | 0.0516       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.741       |\n",
            "|    explained_variance   | 0.624        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.08         |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | 0.00211      |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047330973 |\n",
            "|    clip_fraction        | 0.0521       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.766       |\n",
            "|    explained_variance   | 0.543        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.33         |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | 0.00165      |\n",
            "|    value_loss           | 10.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 106          |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052563883 |\n",
            "|    clip_fraction        | 0.0637       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.816       |\n",
            "|    explained_variance   | 0.447        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.04         |\n",
            "|    n_updates            | 870          |\n",
            "|    policy_gradient_loss | 0.00261      |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 110          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063464046 |\n",
            "|    clip_fraction        | 0.0667       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.861       |\n",
            "|    explained_variance   | 0.448        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.3         |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | 0.00525      |\n",
            "|    value_loss           | 17.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 114          |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038528724 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.799       |\n",
            "|    explained_variance   | 0.54         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.69         |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | 0.000365     |\n",
            "|    value_loss           | 14.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 556         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 117         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002745784 |\n",
            "|    clip_fraction        | 0.036       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.711      |\n",
            "|    explained_variance   | 0.654       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.04        |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | 0.000812    |\n",
            "|    value_loss           | 10.8        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033184974 |\n",
            "|    clip_fraction        | 0.0263       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.785       |\n",
            "|    explained_variance   | 0.558        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.4         |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | 0.000302     |\n",
            "|    value_loss           | 15           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 125          |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0067052636 |\n",
            "|    clip_fraction        | 0.0649       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.744       |\n",
            "|    explained_variance   | 0.727        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.19         |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | 0.000285     |\n",
            "|    value_loss           | 11.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 129          |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042137075 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.71        |\n",
            "|    explained_variance   | 0.713        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.91         |\n",
            "|    n_updates            | 930          |\n",
            "|    policy_gradient_loss | 0.00441      |\n",
            "|    value_loss           | 10.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 132          |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036569657 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.786       |\n",
            "|    explained_variance   | 0.577        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.53         |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | 0.00212      |\n",
            "|    value_loss           | 17           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 136          |\n",
            "|    total_timesteps      | 75776        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054725106 |\n",
            "|    clip_fraction        | 0.0478       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.784       |\n",
            "|    explained_variance   | 0.628        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.37         |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | 0.00292      |\n",
            "|    value_loss           | 16.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 140          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039057015 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.719       |\n",
            "|    explained_variance   | 0.592        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.31         |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.00165     |\n",
            "|    value_loss           | 15.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 143          |\n",
            "|    total_timesteps      | 79872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028116747 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.705       |\n",
            "|    explained_variance   | 0.683        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.62         |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | 0.00253      |\n",
            "|    value_loss           | 11.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 40           |\n",
            "|    time_elapsed         | 147          |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024007156 |\n",
            "|    clip_fraction        | 0.03         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.637       |\n",
            "|    explained_variance   | 0.71         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.48         |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | 0.003        |\n",
            "|    value_loss           | 8.28         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 151          |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033857692 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.732       |\n",
            "|    explained_variance   | 0.576        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.59         |\n",
            "|    n_updates            | 990          |\n",
            "|    policy_gradient_loss | -0.00173     |\n",
            "|    value_loss           | 13.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 555          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 154          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015398185 |\n",
            "|    clip_fraction        | 0.0195       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.771       |\n",
            "|    explained_variance   | 0.687        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.21         |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | 0.000157     |\n",
            "|    value_loss           | 11.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 158          |\n",
            "|    total_timesteps      | 88064        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021253694 |\n",
            "|    clip_fraction        | 0.0179       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.756       |\n",
            "|    explained_variance   | 0.687        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.66         |\n",
            "|    n_updates            | 1010         |\n",
            "|    policy_gradient_loss | -0.00293     |\n",
            "|    value_loss           | 12.9         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 554         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 162         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003207189 |\n",
            "|    clip_fraction        | 0.0583      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.778      |\n",
            "|    explained_variance   | 0.676       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.49        |\n",
            "|    n_updates            | 1020        |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    value_loss           | 9.89        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 555         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 166         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003789852 |\n",
            "|    clip_fraction        | 0.0397      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.877      |\n",
            "|    explained_variance   | 0.489       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.4        |\n",
            "|    n_updates            | 1030        |\n",
            "|    policy_gradient_loss | -0.00302    |\n",
            "|    value_loss           | 18          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 169          |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034505615 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.921       |\n",
            "|    explained_variance   | 0.546        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.14         |\n",
            "|    n_updates            | 1040         |\n",
            "|    policy_gradient_loss | -0.00236     |\n",
            "|    value_loss           | 14.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 554          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 173          |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047022207 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.882       |\n",
            "|    explained_variance   | 0.553        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.51         |\n",
            "|    n_updates            | 1050         |\n",
            "|    policy_gradient_loss | -0.000282    |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 555         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 177         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003932597 |\n",
            "|    clip_fraction        | 0.0402      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.862      |\n",
            "|    explained_variance   | 0.533       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.35        |\n",
            "|    n_updates            | 1060        |\n",
            "|    policy_gradient_loss | 0.00253     |\n",
            "|    value_loss           | 14.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 555         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 180         |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008821119 |\n",
            "|    clip_fraction        | 0.0841      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.88       |\n",
            "|    explained_variance   | 0.491       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.06        |\n",
            "|    n_updates            | 1070        |\n",
            "|    policy_gradient_loss | -0.00128    |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "Training finished.\n",
            "Trained model saved to /content/drive/MyDrive/Colab_Thesis_Models/thesis_policy.pt.zip\n",
            "Adding a small delay after saving...\n",
            "Delay finished.\n",
            "Training environment closed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the OpenAI library\n",
        "from openai import OpenAI\n",
        "# Used to securely store your API key - uncomment if using Colab Secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get your OpenAI API key securely\n",
        "# Or if using Colab Secrets:\n",
        "openai_api_key_secure = userdata.get('OPENAI_API_KEY')\n",
        "openai_organization = userdata.get('OPENAI_ORGANIZATION')\n",
        "openai_project = userdata.get('OPENAI_PROJECT_ID')\n",
        "\n",
        "# Set your project API key\n",
        "OpenAI.api_key = openai_api_key_secure\n",
        "# You must also set organization and project ID\n",
        "OpenAI.organization = openai_organization\n",
        "OpenAI.project = openai_project\n",
        "\n",
        "# Create the OpenAI client\n",
        "client = OpenAI(api_key= OpenAI.api_key)\n",
        "\n",
        "print(\"OpenAI client initialized.\")\n",
        "# Test the client connection (optional)\n",
        "try:\n",
        "    # Use the globally available client object\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\", # Using a suitable model for testing\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "      max_tokens=10\n",
        "    )\n",
        "    print(f\"OpenAI client connected successfully. Response to 'Hallo!': {response.choices[0].message.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to OpenAI client: {e}\")\n",
        "# --- LangGraph Simulation/Inference (using Trained Policy and Real Client) ---\n",
        "print(\"\\n--- Starting LangGraph Simulation with Trained Policy and Real Client ---\")\n",
        "\n",
        "# Load the trained model for INFERENCE *once* before the stream\n",
        "# This model object will be passed to the policy_node via the config argument of app.stream\n",
        "inference_model = None\n",
        "inference_model_path = MODEL_PATH # Use the Drive path with .zip\n",
        "\n",
        "print(f\"Simulation block checking for model at: {inference_model_path}\") # Debug print\n",
        "if os.path.exists(inference_model_path): # Check for the existence of the file\n",
        "    print(f\"Model file found at {inference_model_path} for simulation. Attempting to load.\") # Debug print\n",
        "    try:\n",
        "        # Load with a minimal env instance to ensure observation/action space is correct\n",
        "        temp_env_instance = ThesisEnv() # Use default init (real client implicitly)\n",
        "        temp_env = DummyVecEnv([lambda: temp_env_instance])\n",
        "        # Load using the path that includes .zip\n",
        "        inference_model = PPO.load(inference_model_path, env=temp_env, device=\"auto\", custom_objects={\"StateEncoder\": StateEncoder})\n",
        "        print(\"Trained model loaded successfully for inference.\")\n",
        "        temp_env.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading trained model for inference from {inference_model_path}: {e}. Simulation will use a random policy.\")\n",
        "        inference_model = None\n",
        "else:\n",
        "     print(f\"Trained model not found at {inference_model_path} for inference. Simulation will use a random policy.\")\n",
        "     inference_model = None\n",
        "\n",
        "\n",
        "# Initialize initial_state for simulation\n",
        "initial_days = random.randint(100, 500) # Simulate a deadline range for initial state\n",
        "initial_state = GraphState(\n",
        "    user_input=\"Start thesis writing process.\",\n",
        "    request_type=\"initial_request\",\n",
        "    thesis_stage=\"planning\",\n",
        "    context={\"step_count\": 0, \"initial_days_to_deadline\": initial_days}, # Store initial days in context\n",
        "    personal_value=np.random.uniform(0.4, 0.6),\n",
        "    quality=np.random.uniform(0.4, 0.6),\n",
        "    quantity=0.0,\n",
        "    emotion_state=np.random.uniform(0.4, 0.6),\n",
        "    deadline_ratio=0.0,\n",
        "    # Initialize new user/thesis fields\n",
        "    thesis_field=random.choice([\"Computer Science\", \"Psychology\", \"History\"]),\n",
        "    procrastination_level=random.choice([\"low\", \"medium\", \"high\"]),\n",
        "    writing_style=random.choice([\"concise\", \"descriptive\", \"neutral\"]),\n",
        "    days_to_deadline=initial_days\n",
        ")\n",
        "\n",
        "# Pass the global real client to the initial state context for the LLM node\n",
        "global client # Ensure global client is accessible\n",
        "initial_state.context[\"llm_client\"] = client\n",
        "\n",
        "# Pass the loaded inference_model via the 'config' argument of app.stream\n",
        "# It's accessed via config.get(\"configurable\", {}).get(\"ppo_model\") in the node\n",
        "# This reverts the policy node back to not loading internally\n",
        "simulation_config = {\"recursion_limit\": 500, \"configurable\": {\"ppo_model\": inference_model}}\n",
        "\n",
        "\n",
        "try:\n",
        "    max_simulation_steps = 1\n",
        "    step_count = 0\n",
        "    current_state_dict = initial_state.model_dump()\n",
        "\n",
        "    print(\"Streaming graph execution updates:\")\n",
        "    for step_update in app.stream(initial_state, simulation_config): # Pass the config here\n",
        "        for node_name, update_data in step_update.items():\n",
        "            # Update current_state_dict by merging updates\n",
        "            for key, value in update_data.items():\n",
        "                 if key == 'context':\n",
        "                      current_state_dict['context'].update(value)\n",
        "                 else:\n",
        "                      current_state_dict[key] = value\n",
        "\n",
        "            if node_name == \"core_update_node\":\n",
        "                 step_count = current_state_dict['context'].get('step_count', 0)\n",
        "                 print(f\"\\n--- Step {step_count} ---\")\n",
        "                 print(f\"State: PV={current_state_dict['personal_value']:.2f}, QL={current_state_dict['quality']:.2f}, QT={current_state_dict['quantity']:.2f}, Deadline={current_state_dict['deadline_ratio']:.2f}, Days Left={current_state_dict['days_to_deadline']}\")\n",
        "                 print(f\"Advisor Suggestion: {current_state_dict.get('module_response', 'N/A')}\")\n",
        "\n",
        "        if step_count >= max_simulation_steps:\n",
        "            print(f\"\\n--- Reached maximum simulation steps ({max_simulation_steps}) ---\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n--- Simulation Finished ---\")\n",
        "    final_state_dict = current_state_dict\n",
        "    print(f\"Final State: PV={final_state_dict['personal_value']:.2f}, QL={final_state_dict['quality']:.2f}, QT={final_state_dict['quantity']:.2f}, Steps={final_state_dict['context'].get('step_count', 'N/A')}, Deadline={final_state_dict['deadline_ratio']:.2f}, Days Left={final_state_dict['days_to_deadline']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Simulation Ended Due to Error ---\")\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "print(\"\\n--- Combined Cell Execution Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-h2o5mZIlKk",
        "outputId": "8f1bd5a2-e0d9-4ed9-889e-283da308a74f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized.\n",
            "OpenAI client connected successfully. Response to 'Hallo!': Hello! How can I assist you today?\n",
            "\n",
            "--- Starting LangGraph Simulation with Trained Policy and Real Client ---\n",
            "Simulation block checking for model at: /content/drive/MyDrive/Colab_Thesis_Models/thesis_policy.pt.zip\n",
            "Model file found at /content/drive/MyDrive/Colab_Thesis_Models/thesis_policy.pt.zip for simulation. Attempting to load.\n",
            "Trained model loaded successfully for inference.\n",
            "Streaming graph execution updates:\n",
            "\n",
            "--- Executing Policy Node ---\n",
            "PPO model found in config. Proceeding with prediction.\n",
            "Policy Node Output: Recommended Action Index = 4 (Advisor feedback reminder)\n",
            "\n",
            "--- Executing LLM Abstraction Node (using OpenAI) ---\n",
            "Using client from state context for LLM node (likely mock).\n",
            "\n",
            "--- Executing Core Update Node in 'simulate' mode ---\n",
            "State after update: PV=0.60, QL=0.56, QT=0.00, Steps=1, Deadline=0.00, Days Left=305\n",
            "\n",
            "--- Executing Check Exit Node ---\n",
            "Checking exit conditions: Step Count = 1, Personal Value = 0.60, Quantity = 0.00, Days Left = 305\n",
            "Exit conditions not met. Continuing.\n",
            "\n",
            "--- Step 1 ---\n",
            "State: PV=0.60, QL=0.56, QT=0.00, Deadline=0.00, Days Left=305\n",
            "Advisor Suggestion: It's great that you're aware of the need for feedback—this is a crucial step in overcoming procrastination. Reach out to your advisor this week to schedule a feedback session; they can provide valuable insights to boost your confidence and help you refine your ideas. Remember, taking small steps now can lead to significant progress by your deadline!\n",
            "\n",
            "--- Reached maximum simulation steps (1) ---\n",
            "\n",
            "--- Simulation Finished ---\n",
            "Final State: PV=0.60, QL=0.56, QT=0.00, Steps=1, Deadline=0.00, Days Left=305\n",
            "\n",
            "--- Combined Cell Execution Complete ---\n"
          ]
        }
      ]
    }
  ]
}